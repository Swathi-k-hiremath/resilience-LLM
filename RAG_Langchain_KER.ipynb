{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072683ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a2fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "\n",
    "model_name = \"TheBloke/Llama-2-13b-Chat-GPTQ\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "gen_cfg = GenerationConfig.from_pretrained(model_name)\n",
    "gen_cfg.max_new_tokens=512\n",
    "gen_cfg.temperature=0.0000001 # 0.0\n",
    "gen_cfg.return_full_text=True\n",
    "gen_cfg.do_sample=True\n",
    "gen_cfg.repetition_penalty=1.11\n",
    "\n",
    "pipe=pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    generation_config=gen_cfg\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8e820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import fill\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "[INST] <>\n",
    "You are an AI assistant. You are truthful, unbiased and honest in your response.\n",
    "\n",
    "If you are unsure about an answer, truthfully say \"I don't know\"\n",
    "<>\n",
    "\n",
    "{text} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "text = \"Explain artificial intelligence in a few lines\"\n",
    "result = llm.invoke(prompt.format(text=text))\n",
    "print(fill(result.strip(), width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbf9655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c37233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3860ac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'db/'\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd51b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#persist_directory = 'db2/'\n",
    "#vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85823722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef2ef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "[INST] <>\n",
    "Use the following context to answer the question at the end. Do not use any other information. \n",
    "If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
    "Keep your answer short but descriptive. \n",
    "<>\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "Chain_pdf = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    # retriever=db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 5, 'score_threshold': 0.8})\n",
    "    # Similarity Search is the default way to retrieve documents relevant to a query, but we can use MMR by setting search_type = \"mmr\"\n",
    "    # k defines how many documents are returned; defaults to 4.\n",
    "    # score_threshold allows to set a minimum relevance for documents returned by the retriever, if we are using the \"similarity_score_threshold\" search type.\n",
    "    # return_source_documents=True, # Optional parameter, returns the source documents used to answer the question\n",
    "    retriever=vectordb.as_retriever(), # (search_kwargs={'k': 5, 'score_threshold': 0.8}),\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e6d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain the concept of heat equity\"\n",
    "result = Chain_pdf.invoke(query)\n",
    "print(fill(result['result'].strip(), width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8eb22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who are some vulnerable populations in Arizona \"\n",
    "result = Chain_pdf.invoke(query)\n",
    "print(fill(result['result'].strip(), width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300a56e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the need for a Chief Heat Officer\"\n",
    "result = Chain_pdf.invoke(query)\n",
    "print(fill(result['result'].strip(), width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034d586d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bf0ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122b7bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "[INST] <>\n",
    "Use the following context to answer the question at the end. Do not use any other information. \n",
    "If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
    "Keep your answer short but descriptive. \n",
    "\n",
    "<>\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "refine = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                 chain_type=\"map_reduce\",\n",
    "                                 return_source_documents=True,\n",
    "                                 chain_type_kwargs=chain_type_kwargs,\n",
    "                                 retriever=retriever=vectordb.as_retriever(),\n",
    "                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9da3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain the concept of heat equity\"\n",
    "result = refine.invoke(query)\n",
    "print(fill(result['result'].strip(), width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f31cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who are some vulnerable populations in Arizona \"\n",
    "result = refine.invoke(query)\n",
    "print(fill(result['result'].strip(), width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b6b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the need for a Chief Heat Officer\"\n",
    "result = refine.invoke(query)\n",
    "print(fill(result['result'].strip(), width=100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
